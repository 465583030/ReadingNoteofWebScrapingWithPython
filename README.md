环境：python3.4.3

对于《scraping with python》每一章节最有用的东西的提取以及补充

第一章：最简单的用urllib.request中的urlopen配合BeautifulSoup提取某页面的HTML

第二章：如何用BeautifulSoup定位到想要的内容，如何配合正着表达式使用，如何获取标签中的属性

第三章：实际演示了通过一个页面中的link连续爬取，涉及到了防止爬取重复等问题

第四章：使用网站既有的API来获取数据

第五章：保存数据的问题，保存URL还是下载下来？保存到Mysql，Email等等

第六章：如果不是要爬取HTML而是读取服务器的如TXT，PDF文件应该如何弄

第七章：数据清理（如替换连续空格，替换连续空行，去掉非英文内容），以及第三方工具介绍

第八章：关于自然语言处理中的n-gram ananysis

第九章：提交表单，使用COOKIE，使用SEESION

第十章：使用selenium以及PhantomJS搞定Javascript

第十一章：验证码的识别

第十二章：防止被服务器认定为机器人的措施小结
